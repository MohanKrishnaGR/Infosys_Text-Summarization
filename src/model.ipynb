{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BartTokenizer\n",
    "\n",
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_length=512):\n",
    "        self.dataset = pd.read_csv(file_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset.iloc[idx, 0]\n",
    "        summary = self.dataset.iloc[idx, 1]\n",
    "        \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        targets = self.tokenizer.encode_plus(\n",
    "            summary,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'labels': targets['input_ids'].flatten()\n",
    "        }\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "\n",
    "train_dataset = SummarizationDataset('/home/mohan/infy/data/merged/final/train.csv', tokenizer)\n",
    "val_dataset = SummarizationDataset('/home/mohan/infy/data/merged/final/validation.csv', tokenizer)\n",
    "test_dataset = SummarizationDataset('/home/mohan/infy/data/merged/final/test.csv', tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85dd5a64294c4731b48f1f4fbe5903da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37626 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Epoch 1 | Step 1/12542 | Batch Loss: 13.9620 | Learning Rate: 0.001000 | Batch Time: 1.29s\n",
      "Epoch 1 | Step 2/12542 | Batch Loss: 15.4104 | Learning Rate: 0.001000 | Batch Time: 0.89s\n",
      "Epoch 1 | Step 3/12542 | Batch Loss: 15.1106 | Learning Rate: 0.001000 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 4/12542 | Batch Loss: 16.2638 | Learning Rate: 0.001000 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 5/12542 | Batch Loss: 12.4789 | Learning Rate: 0.001000 | Batch Time: 0.75s\n",
      "Epoch 1 | Step 6/12542 | Batch Loss: 11.4378 | Learning Rate: 0.001000 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 7/12542 | Batch Loss: 10.5441 | Learning Rate: 0.001000 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 8/12542 | Batch Loss: 9.8666 | Learning Rate: 0.001000 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 9/12542 | Batch Loss: 44.8217 | Learning Rate: 0.001000 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 10/12542 | Batch Loss: 8.7507 | Learning Rate: 0.001000 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 11/12542 | Batch Loss: 7.2297 | Learning Rate: 0.001000 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 12/12542 | Batch Loss: 6.1780 | Learning Rate: 0.001000 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 13/12542 | Batch Loss: 5.5539 | Learning Rate: 0.001000 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 14/12542 | Batch Loss: 3.4740 | Learning Rate: 0.001000 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 15/12542 | Batch Loss: 2.6501 | Learning Rate: 0.001000 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 16/12542 | Batch Loss: 3.2877 | Learning Rate: 0.001000 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 17/12542 | Batch Loss: 1.4929 | Learning Rate: 0.001000 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 18/12542 | Batch Loss: 2.7919 | Learning Rate: 0.001000 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 19/12542 | Batch Loss: 2.9406 | Learning Rate: 0.000999 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 20/12542 | Batch Loss: 2.6706 | Learning Rate: 0.000999 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 21/12542 | Batch Loss: 4.0532 | Learning Rate: 0.000999 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 22/12542 | Batch Loss: 5.5566 | Learning Rate: 0.000999 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 23/12542 | Batch Loss: 12.0033 | Learning Rate: 0.000999 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 24/12542 | Batch Loss: 2.3206 | Learning Rate: 0.000999 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 25/12542 | Batch Loss: 2.3679 | Learning Rate: 0.000999 | Batch Time: 0.74s\n",
      "Epoch 1 | Step 26/12542 | Batch Loss: 1.8780 | Learning Rate: 0.000999 | Batch Time: 0.74s\n",
      "Epoch 1 | Step 27/12542 | Batch Loss: 1.2428 | Learning Rate: 0.000999 | Batch Time: 0.74s\n",
      "Epoch 1 | Step 28/12542 | Batch Loss: 2.3567 | Learning Rate: 0.000999 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 29/12542 | Batch Loss: 2.9522 | Learning Rate: 0.000999 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 30/12542 | Batch Loss: 1.3663 | Learning Rate: 0.000999 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 31/12542 | Batch Loss: 1.5479 | Learning Rate: 0.000999 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 32/12542 | Batch Loss: 3.1457 | Learning Rate: 0.000999 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 33/12542 | Batch Loss: 1.3666 | Learning Rate: 0.000999 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 34/12542 | Batch Loss: 1.4316 | Learning Rate: 0.000999 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 35/12542 | Batch Loss: 2.0351 | Learning Rate: 0.000999 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 36/12542 | Batch Loss: 4.0572 | Learning Rate: 0.000999 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 37/12542 | Batch Loss: 2.0887 | Learning Rate: 0.000999 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 38/12542 | Batch Loss: 1.5469 | Learning Rate: 0.000999 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 39/12542 | Batch Loss: 1.5752 | Learning Rate: 0.000999 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 40/12542 | Batch Loss: 2.0773 | Learning Rate: 0.000999 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 41/12542 | Batch Loss: 1.6443 | Learning Rate: 0.000999 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 42/12542 | Batch Loss: 1.4600 | Learning Rate: 0.000999 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 43/12542 | Batch Loss: 1.8653 | Learning Rate: 0.000999 | Batch Time: 0.74s\n",
      "Epoch 1 | Step 44/12542 | Batch Loss: 4.6606 | Learning Rate: 0.000999 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 45/12542 | Batch Loss: 1.3562 | Learning Rate: 0.000999 | Batch Time: 0.74s\n",
      "Epoch 1 | Step 46/12542 | Batch Loss: 2.7580 | Learning Rate: 0.000999 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 47/12542 | Batch Loss: 1.8652 | Learning Rate: 0.000999 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 48/12542 | Batch Loss: 0.8802 | Learning Rate: 0.000999 | Batch Time: 0.74s\n",
      "Epoch 1 | Step 49/12542 | Batch Loss: 2.5587 | Learning Rate: 0.000999 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 50/12542 | Batch Loss: 2.1213 | Learning Rate: 0.000999 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 51/12542 | Batch Loss: 1.5096 | Learning Rate: 0.000999 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 52/12542 | Batch Loss: 2.3503 | Learning Rate: 0.000999 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 53/12542 | Batch Loss: 2.0221 | Learning Rate: 0.000999 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 54/12542 | Batch Loss: 1.7765 | Learning Rate: 0.000999 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 55/12542 | Batch Loss: 2.2760 | Learning Rate: 0.000999 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 56/12542 | Batch Loss: 2.0247 | Learning Rate: 0.000999 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 57/12542 | Batch Loss: 1.8010 | Learning Rate: 0.000998 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 58/12542 | Batch Loss: 1.4396 | Learning Rate: 0.000998 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 59/12542 | Batch Loss: 1.8202 | Learning Rate: 0.000998 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 60/12542 | Batch Loss: 1.4308 | Learning Rate: 0.000998 | Batch Time: 0.74s\n",
      "Epoch 1 | Step 61/12542 | Batch Loss: 2.9940 | Learning Rate: 0.000998 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 62/12542 | Batch Loss: 1.2380 | Learning Rate: 0.000998 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 63/12542 | Batch Loss: 2.1169 | Learning Rate: 0.000998 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 64/12542 | Batch Loss: 1.2096 | Learning Rate: 0.000998 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 65/12542 | Batch Loss: 1.7746 | Learning Rate: 0.000998 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 66/12542 | Batch Loss: 3.3258 | Learning Rate: 0.000998 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 67/12542 | Batch Loss: 1.3473 | Learning Rate: 0.000998 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 68/12542 | Batch Loss: 1.0033 | Learning Rate: 0.000998 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 69/12542 | Batch Loss: 1.3233 | Learning Rate: 0.000998 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 70/12542 | Batch Loss: 2.2331 | Learning Rate: 0.000998 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 71/12542 | Batch Loss: 1.6979 | Learning Rate: 0.000998 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 72/12542 | Batch Loss: 1.3860 | Learning Rate: 0.000998 | Batch Time: 0.74s\n",
      "Epoch 1 | Step 73/12542 | Batch Loss: 2.4427 | Learning Rate: 0.000998 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 74/12542 | Batch Loss: 2.3448 | Learning Rate: 0.000998 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 75/12542 | Batch Loss: 2.0221 | Learning Rate: 0.000998 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 76/12542 | Batch Loss: 1.4359 | Learning Rate: 0.000998 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 77/12542 | Batch Loss: 3.4209 | Learning Rate: 0.000998 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 78/12542 | Batch Loss: 1.0769 | Learning Rate: 0.000998 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 79/12542 | Batch Loss: 1.8526 | Learning Rate: 0.000998 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 80/12542 | Batch Loss: 1.9372 | Learning Rate: 0.000998 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 81/12542 | Batch Loss: 1.6027 | Learning Rate: 0.000998 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 82/12542 | Batch Loss: 2.3287 | Learning Rate: 0.000998 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 83/12542 | Batch Loss: 1.3161 | Learning Rate: 0.000998 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 84/12542 | Batch Loss: 1.4998 | Learning Rate: 0.000998 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 85/12542 | Batch Loss: 2.2712 | Learning Rate: 0.000998 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 86/12542 | Batch Loss: 1.5343 | Learning Rate: 0.000998 | Batch Time: 0.81s\n",
      "Epoch 1 | Step 87/12542 | Batch Loss: 2.0966 | Learning Rate: 0.000998 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 88/12542 | Batch Loss: 0.8179 | Learning Rate: 0.000998 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 89/12542 | Batch Loss: 3.3821 | Learning Rate: 0.000998 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 90/12542 | Batch Loss: 1.3166 | Learning Rate: 0.000998 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 91/12542 | Batch Loss: 1.7206 | Learning Rate: 0.000998 | Batch Time: 0.74s\n",
      "Epoch 1 | Step 92/12542 | Batch Loss: 1.4930 | Learning Rate: 0.000998 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 93/12542 | Batch Loss: 1.2425 | Learning Rate: 0.000998 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 94/12542 | Batch Loss: 2.4248 | Learning Rate: 0.000998 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 95/12542 | Batch Loss: 2.1809 | Learning Rate: 0.000997 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 96/12542 | Batch Loss: 1.7873 | Learning Rate: 0.000997 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 97/12542 | Batch Loss: 1.4873 | Learning Rate: 0.000997 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 98/12542 | Batch Loss: 1.7119 | Learning Rate: 0.000997 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 99/12542 | Batch Loss: 3.2468 | Learning Rate: 0.000997 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 100/12542 | Batch Loss: 1.9869 | Learning Rate: 0.000997 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 101/12542 | Batch Loss: 2.4564 | Learning Rate: 0.000997 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 102/12542 | Batch Loss: 3.0941 | Learning Rate: 0.000997 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 103/12542 | Batch Loss: 3.0457 | Learning Rate: 0.000997 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 104/12542 | Batch Loss: 2.1747 | Learning Rate: 0.000997 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 105/12542 | Batch Loss: 1.4969 | Learning Rate: 0.000997 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 106/12542 | Batch Loss: 0.8585 | Learning Rate: 0.000997 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 107/12542 | Batch Loss: 1.2346 | Learning Rate: 0.000997 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 108/12542 | Batch Loss: 0.8000 | Learning Rate: 0.000997 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 109/12542 | Batch Loss: 1.5334 | Learning Rate: 0.000997 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 110/12542 | Batch Loss: 1.3110 | Learning Rate: 0.000997 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 111/12542 | Batch Loss: 2.3114 | Learning Rate: 0.000997 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 112/12542 | Batch Loss: 1.5856 | Learning Rate: 0.000997 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 113/12542 | Batch Loss: 2.0520 | Learning Rate: 0.000997 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 114/12542 | Batch Loss: 1.8012 | Learning Rate: 0.000997 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 115/12542 | Batch Loss: 2.1291 | Learning Rate: 0.000997 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 116/12542 | Batch Loss: 1.9903 | Learning Rate: 0.000997 | Batch Time: 0.75s\n",
      "Epoch 1 | Step 117/12542 | Batch Loss: 1.4772 | Learning Rate: 0.000997 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 118/12542 | Batch Loss: 1.4405 | Learning Rate: 0.000997 | Batch Time: 0.74s\n",
      "Epoch 1 | Step 119/12542 | Batch Loss: 2.0140 | Learning Rate: 0.000997 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 120/12542 | Batch Loss: 1.0561 | Learning Rate: 0.000997 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 121/12542 | Batch Loss: 0.7863 | Learning Rate: 0.000997 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 122/12542 | Batch Loss: 2.8933 | Learning Rate: 0.000997 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 123/12542 | Batch Loss: 1.1487 | Learning Rate: 0.000997 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 124/12542 | Batch Loss: 1.4662 | Learning Rate: 0.000997 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 125/12542 | Batch Loss: 1.6007 | Learning Rate: 0.000997 | Batch Time: 0.71s\n",
      "Epoch 1 | Step 126/12542 | Batch Loss: 1.7195 | Learning Rate: 0.000997 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 127/12542 | Batch Loss: 1.6263 | Learning Rate: 0.000997 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 128/12542 | Batch Loss: 1.5679 | Learning Rate: 0.000997 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 129/12542 | Batch Loss: 0.7309 | Learning Rate: 0.000997 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 130/12542 | Batch Loss: 1.5799 | Learning Rate: 0.000997 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 131/12542 | Batch Loss: 1.2929 | Learning Rate: 0.000997 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 132/12542 | Batch Loss: 3.1461 | Learning Rate: 0.000996 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 133/12542 | Batch Loss: 1.0155 | Learning Rate: 0.000996 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 134/12542 | Batch Loss: 2.3156 | Learning Rate: 0.000996 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 135/12542 | Batch Loss: 3.8449 | Learning Rate: 0.000996 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 136/12542 | Batch Loss: 1.9363 | Learning Rate: 0.000996 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 137/12542 | Batch Loss: 2.1262 | Learning Rate: 0.000996 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 138/12542 | Batch Loss: 1.1829 | Learning Rate: 0.000996 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 139/12542 | Batch Loss: 1.0648 | Learning Rate: 0.000996 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 140/12542 | Batch Loss: 1.7065 | Learning Rate: 0.000996 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 141/12542 | Batch Loss: 1.7188 | Learning Rate: 0.000996 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 142/12542 | Batch Loss: 2.8201 | Learning Rate: 0.000996 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 143/12542 | Batch Loss: 1.7409 | Learning Rate: 0.000996 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 144/12542 | Batch Loss: 1.1517 | Learning Rate: 0.000996 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 145/12542 | Batch Loss: 0.6091 | Learning Rate: 0.000996 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 146/12542 | Batch Loss: 1.8313 | Learning Rate: 0.000996 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 147/12542 | Batch Loss: 0.9820 | Learning Rate: 0.000996 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 148/12542 | Batch Loss: 0.7716 | Learning Rate: 0.000996 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 149/12542 | Batch Loss: 2.4293 | Learning Rate: 0.000996 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 150/12542 | Batch Loss: 1.7101 | Learning Rate: 0.000996 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 151/12542 | Batch Loss: 1.9586 | Learning Rate: 0.000996 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 152/12542 | Batch Loss: 1.1833 | Learning Rate: 0.000996 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 153/12542 | Batch Loss: 2.8737 | Learning Rate: 0.000996 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 154/12542 | Batch Loss: 1.3894 | Learning Rate: 0.000996 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 155/12542 | Batch Loss: 2.1760 | Learning Rate: 0.000996 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 156/12542 | Batch Loss: 1.9965 | Learning Rate: 0.000996 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 157/12542 | Batch Loss: 1.1395 | Learning Rate: 0.000996 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 158/12542 | Batch Loss: 2.8614 | Learning Rate: 0.000996 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 159/12542 | Batch Loss: 1.7451 | Learning Rate: 0.000996 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 160/12542 | Batch Loss: 0.9844 | Learning Rate: 0.000996 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 161/12542 | Batch Loss: 0.9322 | Learning Rate: 0.000996 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 162/12542 | Batch Loss: 1.8127 | Learning Rate: 0.000996 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 163/12542 | Batch Loss: 1.2633 | Learning Rate: 0.000996 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 164/12542 | Batch Loss: 0.9358 | Learning Rate: 0.000996 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 165/12542 | Batch Loss: 1.3581 | Learning Rate: 0.000996 | Batch Time: 0.74s\n",
      "Epoch 1 | Step 166/12542 | Batch Loss: 2.0547 | Learning Rate: 0.000996 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 167/12542 | Batch Loss: 1.5586 | Learning Rate: 0.000996 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 168/12542 | Batch Loss: 2.0537 | Learning Rate: 0.000996 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 169/12542 | Batch Loss: 1.4209 | Learning Rate: 0.000996 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 170/12542 | Batch Loss: 1.8852 | Learning Rate: 0.000995 | Batch Time: 0.74s\n",
      "Epoch 1 | Step 171/12542 | Batch Loss: 1.9959 | Learning Rate: 0.000995 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 172/12542 | Batch Loss: 1.7313 | Learning Rate: 0.000995 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 173/12542 | Batch Loss: 1.9870 | Learning Rate: 0.000995 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 174/12542 | Batch Loss: 2.0069 | Learning Rate: 0.000995 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 175/12542 | Batch Loss: 1.3834 | Learning Rate: 0.000995 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 176/12542 | Batch Loss: 2.1114 | Learning Rate: 0.000995 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 177/12542 | Batch Loss: 1.0256 | Learning Rate: 0.000995 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 178/12542 | Batch Loss: 1.8823 | Learning Rate: 0.000995 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 179/12542 | Batch Loss: 1.4189 | Learning Rate: 0.000995 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 180/12542 | Batch Loss: 1.0672 | Learning Rate: 0.000995 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 181/12542 | Batch Loss: 1.9049 | Learning Rate: 0.000995 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 182/12542 | Batch Loss: 2.2793 | Learning Rate: 0.000995 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 183/12542 | Batch Loss: 2.1655 | Learning Rate: 0.000995 | Batch Time: 0.74s\n",
      "Epoch 1 | Step 184/12542 | Batch Loss: 1.4913 | Learning Rate: 0.000995 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 185/12542 | Batch Loss: 2.4628 | Learning Rate: 0.000995 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 186/12542 | Batch Loss: 1.7115 | Learning Rate: 0.000995 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 187/12542 | Batch Loss: 1.7061 | Learning Rate: 0.000995 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 188/12542 | Batch Loss: 1.3081 | Learning Rate: 0.000995 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 189/12542 | Batch Loss: 1.1090 | Learning Rate: 0.000995 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 190/12542 | Batch Loss: 0.9315 | Learning Rate: 0.000995 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 191/12542 | Batch Loss: 0.9537 | Learning Rate: 0.000995 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 192/12542 | Batch Loss: 1.0037 | Learning Rate: 0.000995 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 193/12542 | Batch Loss: 1.9389 | Learning Rate: 0.000995 | Batch Time: 0.74s\n",
      "Epoch 1 | Step 194/12542 | Batch Loss: 2.2358 | Learning Rate: 0.000995 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 195/12542 | Batch Loss: 2.0559 | Learning Rate: 0.000995 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 196/12542 | Batch Loss: 0.9193 | Learning Rate: 0.000995 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 197/12542 | Batch Loss: 2.1088 | Learning Rate: 0.000995 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 198/12542 | Batch Loss: 4.3908 | Learning Rate: 0.000995 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 199/12542 | Batch Loss: 1.1205 | Learning Rate: 0.000995 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 200/12542 | Batch Loss: 0.7271 | Learning Rate: 0.000995 | Batch Time: 0.71s\n",
      "Epoch 1 | Step 201/12542 | Batch Loss: 1.0220 | Learning Rate: 0.000995 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 202/12542 | Batch Loss: 2.1719 | Learning Rate: 0.000995 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 203/12542 | Batch Loss: 1.2064 | Learning Rate: 0.000995 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 204/12542 | Batch Loss: 3.0219 | Learning Rate: 0.000995 | Batch Time: 0.74s\n",
      "Epoch 1 | Step 205/12542 | Batch Loss: 1.6214 | Learning Rate: 0.000995 | Batch Time: 0.75s\n",
      "Epoch 1 | Step 206/12542 | Batch Loss: 1.0243 | Learning Rate: 0.000995 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 207/12542 | Batch Loss: 1.0579 | Learning Rate: 0.000994 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 208/12542 | Batch Loss: 1.8109 | Learning Rate: 0.000994 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 209/12542 | Batch Loss: 1.3786 | Learning Rate: 0.000994 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 210/12542 | Batch Loss: 1.8671 | Learning Rate: 0.000994 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 211/12542 | Batch Loss: 1.3265 | Learning Rate: 0.000994 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 212/12542 | Batch Loss: 2.1694 | Learning Rate: 0.000994 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 213/12542 | Batch Loss: 2.1152 | Learning Rate: 0.000994 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 214/12542 | Batch Loss: 0.9613 | Learning Rate: 0.000994 | Batch Time: 0.74s\n",
      "Epoch 1 | Step 215/12542 | Batch Loss: 1.3562 | Learning Rate: 0.000994 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 216/12542 | Batch Loss: 1.0459 | Learning Rate: 0.000994 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 217/12542 | Batch Loss: 2.2232 | Learning Rate: 0.000994 | Batch Time: 0.83s\n",
      "Epoch 1 | Step 218/12542 | Batch Loss: 1.5662 | Learning Rate: 0.000994 | Batch Time: 0.74s\n",
      "Epoch 1 | Step 219/12542 | Batch Loss: 1.1715 | Learning Rate: 0.000994 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 220/12542 | Batch Loss: 1.3197 | Learning Rate: 0.000994 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 221/12542 | Batch Loss: 2.0293 | Learning Rate: 0.000994 | Batch Time: 0.74s\n",
      "Epoch 1 | Step 222/12542 | Batch Loss: 1.6708 | Learning Rate: 0.000994 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 223/12542 | Batch Loss: 1.0223 | Learning Rate: 0.000994 | Batch Time: 0.74s\n",
      "Epoch 1 | Step 224/12542 | Batch Loss: 1.1115 | Learning Rate: 0.000994 | Batch Time: 0.74s\n",
      "Epoch 1 | Step 225/12542 | Batch Loss: 2.2211 | Learning Rate: 0.000994 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 226/12542 | Batch Loss: 2.1258 | Learning Rate: 0.000994 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 227/12542 | Batch Loss: 1.8391 | Learning Rate: 0.000994 | Batch Time: 0.75s\n",
      "Epoch 1 | Step 228/12542 | Batch Loss: 2.0003 | Learning Rate: 0.000994 | Batch Time: 0.74s\n",
      "Epoch 1 | Step 229/12542 | Batch Loss: 3.6083 | Learning Rate: 0.000994 | Batch Time: 0.74s\n",
      "Epoch 1 | Step 230/12542 | Batch Loss: 2.0201 | Learning Rate: 0.000994 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 231/12542 | Batch Loss: 1.2500 | Learning Rate: 0.000994 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 232/12542 | Batch Loss: 1.1941 | Learning Rate: 0.000994 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 233/12542 | Batch Loss: 1.9194 | Learning Rate: 0.000994 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 234/12542 | Batch Loss: 1.0947 | Learning Rate: 0.000994 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 235/12542 | Batch Loss: 2.6468 | Learning Rate: 0.000994 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 236/12542 | Batch Loss: 1.2172 | Learning Rate: 0.000994 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 237/12542 | Batch Loss: 1.0049 | Learning Rate: 0.000994 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 238/12542 | Batch Loss: 1.6367 | Learning Rate: 0.000994 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 239/12542 | Batch Loss: 1.4814 | Learning Rate: 0.000994 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 240/12542 | Batch Loss: 1.7321 | Learning Rate: 0.000994 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 241/12542 | Batch Loss: 1.2445 | Learning Rate: 0.000994 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 242/12542 | Batch Loss: 0.9697 | Learning Rate: 0.000994 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 243/12542 | Batch Loss: 1.3560 | Learning Rate: 0.000994 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 244/12542 | Batch Loss: 1.4648 | Learning Rate: 0.000994 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 245/12542 | Batch Loss: 1.3431 | Learning Rate: 0.000993 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 246/12542 | Batch Loss: 5.5284 | Learning Rate: 0.000993 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 247/12542 | Batch Loss: 1.2462 | Learning Rate: 0.000993 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 248/12542 | Batch Loss: 1.1894 | Learning Rate: 0.000993 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 249/12542 | Batch Loss: 1.8115 | Learning Rate: 0.000993 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 250/12542 | Batch Loss: 1.6023 | Learning Rate: 0.000993 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 251/12542 | Batch Loss: 0.7594 | Learning Rate: 0.000993 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 252/12542 | Batch Loss: 1.0086 | Learning Rate: 0.000993 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 253/12542 | Batch Loss: 2.4648 | Learning Rate: 0.000993 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 254/12542 | Batch Loss: 1.8181 | Learning Rate: 0.000993 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 255/12542 | Batch Loss: 1.4633 | Learning Rate: 0.000993 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 256/12542 | Batch Loss: 1.3042 | Learning Rate: 0.000993 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 257/12542 | Batch Loss: 1.4671 | Learning Rate: 0.000993 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 258/12542 | Batch Loss: 2.0313 | Learning Rate: 0.000993 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 259/12542 | Batch Loss: 1.6690 | Learning Rate: 0.000993 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 260/12542 | Batch Loss: 2.6339 | Learning Rate: 0.000993 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 261/12542 | Batch Loss: 1.2548 | Learning Rate: 0.000993 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 262/12542 | Batch Loss: 1.2605 | Learning Rate: 0.000993 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 263/12542 | Batch Loss: 2.5127 | Learning Rate: 0.000993 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 264/12542 | Batch Loss: 0.8024 | Learning Rate: 0.000993 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 265/12542 | Batch Loss: 1.5709 | Learning Rate: 0.000993 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 266/12542 | Batch Loss: 0.8476 | Learning Rate: 0.000993 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 267/12542 | Batch Loss: 1.6590 | Learning Rate: 0.000993 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 268/12542 | Batch Loss: 2.4936 | Learning Rate: 0.000993 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 269/12542 | Batch Loss: 2.5893 | Learning Rate: 0.000993 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 270/12542 | Batch Loss: 0.9150 | Learning Rate: 0.000993 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 271/12542 | Batch Loss: 1.9288 | Learning Rate: 0.000993 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 272/12542 | Batch Loss: 2.0267 | Learning Rate: 0.000993 | Batch Time: 0.74s\n",
      "Epoch 1 | Step 273/12542 | Batch Loss: 0.7240 | Learning Rate: 0.000993 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 274/12542 | Batch Loss: 1.0613 | Learning Rate: 0.000993 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 275/12542 | Batch Loss: 0.8002 | Learning Rate: 0.000993 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 276/12542 | Batch Loss: 2.1261 | Learning Rate: 0.000993 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 277/12542 | Batch Loss: 1.5172 | Learning Rate: 0.000993 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 278/12542 | Batch Loss: 1.7890 | Learning Rate: 0.000993 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 279/12542 | Batch Loss: 1.6234 | Learning Rate: 0.000993 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 280/12542 | Batch Loss: 0.8802 | Learning Rate: 0.000993 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 281/12542 | Batch Loss: 1.4992 | Learning Rate: 0.000993 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 282/12542 | Batch Loss: 2.0050 | Learning Rate: 0.000993 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 283/12542 | Batch Loss: 1.5635 | Learning Rate: 0.000992 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 284/12542 | Batch Loss: 1.2728 | Learning Rate: 0.000992 | Batch Time: 0.74s\n",
      "Epoch 1 | Step 285/12542 | Batch Loss: 3.0638 | Learning Rate: 0.000992 | Batch Time: 0.74s\n",
      "Epoch 1 | Step 286/12542 | Batch Loss: 0.8477 | Learning Rate: 0.000992 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 287/12542 | Batch Loss: 1.2555 | Learning Rate: 0.000992 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 288/12542 | Batch Loss: 1.5706 | Learning Rate: 0.000992 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 289/12542 | Batch Loss: 1.1556 | Learning Rate: 0.000992 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 290/12542 | Batch Loss: 1.7522 | Learning Rate: 0.000992 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 291/12542 | Batch Loss: 2.5468 | Learning Rate: 0.000992 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 292/12542 | Batch Loss: 1.8549 | Learning Rate: 0.000992 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 293/12542 | Batch Loss: 2.1039 | Learning Rate: 0.000992 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 294/12542 | Batch Loss: 1.9414 | Learning Rate: 0.000992 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 295/12542 | Batch Loss: 1.8541 | Learning Rate: 0.000992 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 296/12542 | Batch Loss: 2.2205 | Learning Rate: 0.000992 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 297/12542 | Batch Loss: 1.4088 | Learning Rate: 0.000992 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 298/12542 | Batch Loss: 1.0033 | Learning Rate: 0.000992 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 299/12542 | Batch Loss: 0.9536 | Learning Rate: 0.000992 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 300/12542 | Batch Loss: 1.5540 | Learning Rate: 0.000992 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 301/12542 | Batch Loss: 2.3107 | Learning Rate: 0.000992 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 302/12542 | Batch Loss: 2.6019 | Learning Rate: 0.000992 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 303/12542 | Batch Loss: 1.0278 | Learning Rate: 0.000992 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 304/12542 | Batch Loss: 1.4769 | Learning Rate: 0.000992 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 305/12542 | Batch Loss: 1.7175 | Learning Rate: 0.000992 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 306/12542 | Batch Loss: 0.9743 | Learning Rate: 0.000992 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 307/12542 | Batch Loss: 1.9611 | Learning Rate: 0.000992 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 308/12542 | Batch Loss: 2.9830 | Learning Rate: 0.000992 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 309/12542 | Batch Loss: 0.9291 | Learning Rate: 0.000992 | Batch Time: 0.74s\n",
      "Epoch 1 | Step 310/12542 | Batch Loss: 1.5664 | Learning Rate: 0.000992 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 311/12542 | Batch Loss: 0.9097 | Learning Rate: 0.000992 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 312/12542 | Batch Loss: 1.0043 | Learning Rate: 0.000992 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 313/12542 | Batch Loss: 1.3535 | Learning Rate: 0.000992 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 314/12542 | Batch Loss: 2.5853 | Learning Rate: 0.000992 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 315/12542 | Batch Loss: 1.5252 | Learning Rate: 0.000992 | Batch Time: 0.74s\n",
      "Epoch 1 | Step 316/12542 | Batch Loss: 1.0296 | Learning Rate: 0.000992 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 317/12542 | Batch Loss: 0.7720 | Learning Rate: 0.000992 | Batch Time: 0.73s\n",
      "Epoch 1 | Step 318/12542 | Batch Loss: 2.6722 | Learning Rate: 0.000992 | Batch Time: 0.72s\n",
      "Epoch 1 | Step 319/12542 | Batch Loss: 0.9655 | Learning Rate: 0.000992 | Batch Time: 0.72s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     37\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 38\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/infosys/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:75\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     74\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/infosys/lib/python3.11/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/infosys/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/infosys/lib/python3.11/site-packages/transformers/optimization.py:643\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    639\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;66;03m# In-place operations to update the averages at the same time\u001b[39;00m\n\u001b[0;32m--> 643\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta1))\n\u001b[1;32m    644\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    645\u001b[0m denom \u001b[38;5;241m=\u001b[39m exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt()\u001b[38;5;241m.\u001b[39madd_(group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from transformers import AdamW, get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    epoch_start_time = time.time()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(train_loader):\n",
    "        \n",
    "        batch_start_time = time.time()    \n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=inputs, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "               \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        current_lr = lr_scheduler.get_last_lr()[0]\n",
    "        batch_time = time.time() - batch_start_time\n",
    "        print(f\"Epoch {epoch + 1} | Step {step + 1}/{len(train_loader)} | \"\n",
    "              f\"Batch Loss: {loss.item():.4f} | Learning Rate: {current_lr:.6f} | \"\n",
    "              f\"Batch Time: {batch_time:.2f}s\")\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    print(f\"Epoch {epoch + 1} completed. Average Loss: {avg_loss:.4f} | \"\n",
    "          f\"Epoch Time: {epoch_time:.2f}s\")\n",
    "    \n",
    "total_training_time = time.time() - start_time\n",
    "print(f\"Training completed in {total_training_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "total_eval_loss = 0\n",
    "eval_start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step, batch in enumerate(val_loader):\n",
    "        batch_start_time = time.time()\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=inputs, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "        \n",
    "        batch_time = time.time() - batch_start_time\n",
    "        print(f\"Validation Step {step + 1}/{len(val_loader)} | \"\n",
    "              f\"Batch Loss: {loss.item():.4f} | Batch Time: {batch_time:.2f}s\")\n",
    "\n",
    "avg_eval_loss = total_eval_loss / len(val_loader)\n",
    "\n",
    "eval_time = time.time() - eval_start_time\n",
    "\n",
    "print(f\"Validation Loss: {avg_eval_loss:.4f} | Evaluation Time: {eval_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/home/mohan/infy/models/fine_tuned_bart/tokenizer_config.json',\n",
       " '/home/mohan/infy/models/fine_tuned_bart/special_tokens_map.json',\n",
       " '/home/mohan/infy/models/fine_tuned_bart/vocab.json',\n",
       " '/home/mohan/infy/models/fine_tuned_bart/merges.txt',\n",
       " '/home/mohan/infy/models/fine_tuned_bart/added_tokens.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"/home/mohan/infy/models/fine_tuned_bart\")\n",
    "tokenizer.save_pretrained(\"/home/mohan/infy/models/fine_tuned_bart\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "infosys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
