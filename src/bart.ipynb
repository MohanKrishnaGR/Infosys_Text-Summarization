{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/mohan/miniconda3/envs/env/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (4.41.2)\n",
      "Requirement already satisfied: filelock in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from transformers) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
      "/bin/bash: /home/mohan/miniconda3/envs/env/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: datasets in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (2.19.2)\n",
      "Requirement already satisfied: filelock in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.1 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from datasets) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from datasets) (0.23.2)\n",
      "Requirement already satisfied: packaging in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from requests>=2.32.1->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from requests>=2.32.1->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from requests>=2.32.1->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from requests>=2.32.1->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "/bin/bash: /home/mohan/miniconda3/envs/env/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: tensorboard in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (2.17.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from tensorboard) (1.64.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from tensorboard) (3.6)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from tensorboard) (1.26.4)\n",
      "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from tensorboard) (4.25.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from tensorboard) (69.5.1)\n",
      "Requirement already satisfied: six>1.9 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from tensorboard) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
      "/bin/bash: /home/mohan/miniconda3/envs/env/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: sentencepiece in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (0.2.0)\n",
      "/bin/bash: /home/mohan/miniconda3/envs/env/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: accelerate in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (0.30.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from accelerate) (24.0)\n",
      "Requirement already satisfied: psutil in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from accelerate) (2.3.0)\n",
      "Requirement already satisfied: huggingface-hub in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from accelerate) (0.23.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\n",
      "Requirement already satisfied: requests in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from huggingface-hub->accelerate) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from huggingface-hub->accelerate) (4.66.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "/bin/bash: /home/mohan/miniconda3/envs/env/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: evaluate in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (0.4.2)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from evaluate) (2.19.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from evaluate) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from evaluate) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.3.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from evaluate) (0.23.2)\n",
      "Requirement already satisfied: packaging in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from evaluate) (24.0)\n",
      "Requirement already satisfied: filelock in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: aiohttp in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from pandas->evaluate) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "/bin/bash: /home/mohan/miniconda3/envs/env/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: rouge_score in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: nltk in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: numpy in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from nltk->rouge_score) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in /home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages (from nltk->rouge_score) (4.66.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers\n",
    "!pip install -U datasets\n",
    "!pip install tensorboard\n",
    "!pip install sentencepiece\n",
    "!pip install accelerate\n",
    "!pip install evaluate\n",
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    " \n",
    "import glob\n",
    "import pprint\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    " \n",
    "pp = pprint.PrettyPrinter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text, model, tokenizer, max_length=512, num_beams=5):\n",
    "    # Preprocess the text\n",
    "    inputs = tokenizer.encode(\n",
    "        \"summarize: \" + text,\n",
    "        return_tensors='pt',\n",
    "        max_length=max_length,\n",
    "        truncation=True\n",
    "    )\n",
    " \n",
    "    # Generate the summary\n",
    "    summary_ids = model.generate(\n",
    "        inputs,\n",
    "        max_length=50,\n",
    "        num_beams=num_beams,\n",
    "        # early_stopping=True,\n",
    "    )\n",
    " \n",
    "    # Decode and return the summary\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pprint\n",
    "import evaluate\n",
    "import numpy as np\n",
    " \n",
    "from transformers import (\n",
    "    BartTokenizer,\n",
    "    BartForConditionalGeneration,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "dataset_train = pd.read_csv('/home/mohan/infy/data/fined/split/train.csv')\n",
    "dataset_valid = pd.read_csv('/home/mohan/infy/data/fined/split/validation.csv')\n",
    "\n",
    "df1 = pd.read_csv('/home/mohan/infy/data/fined/split/train.csv')\n",
    "dataset1 = ds.dataset(pa.Table.from_pandas(df1).to_batches())\n",
    "\n",
    "### convert to Huggingface dataset\n",
    "dataset_train = Dataset(pa.Table.from_pandas(df1))\n",
    "print(dataset_train.__class__)\n",
    "\n",
    "df2 = pd.read_csv('/home/mohan/infy/data/fined/split/validation.csv')\n",
    "dataset1 = ds.dataset(pa.Table.from_pandas(df2).to_batches())\n",
    "dataset_valid = Dataset(pa.Table.from_pandas(df2))\n",
    "print(dataset_valid.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'summary'],\n",
      "    num_rows: 50165\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'summary'],\n",
      "    num_rows: 6271\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train)\n",
    "print(dataset_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'facebook/bart-large'\n",
    "BATCH_SIZE = 4\n",
    "NUM_PROCS = 4\n",
    "EPOCHS = 10\n",
    "OUT_DIR = '/home/mohan/infy/models/fine_tuned_Text_Summ'\n",
    "MAX_LENGTH = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0607cb5421e747029260354313810214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/50165 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b090f957c0f4fa389e183c4774208f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/6271 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained(MODEL)\n",
    " \n",
    "# Function to convert text data into model inputs and targets\n",
    "def preprocess_function(examples):\n",
    "    inputs = [f\"summarize: {article}\" for article in examples['text']]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    " \n",
    "    # Set up the tokenizer for targets\n",
    "    targets = [summary for summary in examples['summary']]\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=MAX_LENGTH,\n",
    "            truncation=True,\n",
    "            padding='max_length'\n",
    "        )\n",
    " \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    " \n",
    "# Apply the function to the whole dataset\n",
    "tokenized_train = dataset_train.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=NUM_PROCS\n",
    ")\n",
    "tokenized_valid = dataset_valid.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=NUM_PROCS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406,291,456 total parameters.\n",
      "406,291,456 training parameters.\n"
     ]
    }
   ],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained(MODEL)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    " \n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred.predictions[0], eval_pred.label_ids\n",
    " \n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    " \n",
    "    result = rouge.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True,\n",
    "        rouge_types=[\n",
    "            'rouge1',\n",
    "            'rouge2',\n",
    "            'rougeL'\n",
    "        ]\n",
    "    )\n",
    " \n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    " \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    Original Trainer may have a memory leak.\n",
    "    This is a workaround to avoid storing too many tensors that are not needed.\n",
    "    \"\"\"\n",
    "    pred_ids = torch.argmax(logits[0], dim=-1)\n",
    "    return pred_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohan/miniconda3/envs/infosys/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125420' max='125420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125420/125420 26:24:22, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.563000</td>\n",
       "      <td>0.586864</td>\n",
       "      <td>0.584700</td>\n",
       "      <td>0.292700</td>\n",
       "      <td>0.507500</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.463400</td>\n",
       "      <td>0.573142</td>\n",
       "      <td>0.590800</td>\n",
       "      <td>0.299500</td>\n",
       "      <td>0.515600</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.533200</td>\n",
       "      <td>0.558139</td>\n",
       "      <td>0.591500</td>\n",
       "      <td>0.301600</td>\n",
       "      <td>0.516900</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.601300</td>\n",
       "      <td>0.555438</td>\n",
       "      <td>0.588900</td>\n",
       "      <td>0.296500</td>\n",
       "      <td>0.513600</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.723500</td>\n",
       "      <td>0.547032</td>\n",
       "      <td>0.592400</td>\n",
       "      <td>0.300100</td>\n",
       "      <td>0.518100</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.539100</td>\n",
       "      <td>0.581864</td>\n",
       "      <td>0.540800</td>\n",
       "      <td>0.241900</td>\n",
       "      <td>0.461700</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.668100</td>\n",
       "      <td>0.536584</td>\n",
       "      <td>0.599300</td>\n",
       "      <td>0.309200</td>\n",
       "      <td>0.524700</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.443000</td>\n",
       "      <td>0.533588</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.307900</td>\n",
       "      <td>0.526000</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.408200</td>\n",
       "      <td>0.553599</td>\n",
       "      <td>0.582600</td>\n",
       "      <td>0.287800</td>\n",
       "      <td>0.506800</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.567400</td>\n",
       "      <td>0.526812</td>\n",
       "      <td>0.602000</td>\n",
       "      <td>0.313800</td>\n",
       "      <td>0.528700</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.436400</td>\n",
       "      <td>0.528535</td>\n",
       "      <td>0.600500</td>\n",
       "      <td>0.313600</td>\n",
       "      <td>0.527600</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.738300</td>\n",
       "      <td>0.522426</td>\n",
       "      <td>0.597100</td>\n",
       "      <td>0.308700</td>\n",
       "      <td>0.525800</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.541700</td>\n",
       "      <td>0.521362</td>\n",
       "      <td>0.604900</td>\n",
       "      <td>0.317700</td>\n",
       "      <td>0.532700</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.372900</td>\n",
       "      <td>0.521287</td>\n",
       "      <td>0.608300</td>\n",
       "      <td>0.321000</td>\n",
       "      <td>0.535100</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.406800</td>\n",
       "      <td>0.522895</td>\n",
       "      <td>0.605000</td>\n",
       "      <td>0.320900</td>\n",
       "      <td>0.533700</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.364700</td>\n",
       "      <td>0.516202</td>\n",
       "      <td>0.610600</td>\n",
       "      <td>0.323900</td>\n",
       "      <td>0.537700</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.427700</td>\n",
       "      <td>0.512374</td>\n",
       "      <td>0.612400</td>\n",
       "      <td>0.327800</td>\n",
       "      <td>0.540700</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.459300</td>\n",
       "      <td>0.509248</td>\n",
       "      <td>0.614100</td>\n",
       "      <td>0.329200</td>\n",
       "      <td>0.542600</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.320600</td>\n",
       "      <td>0.517694</td>\n",
       "      <td>0.611300</td>\n",
       "      <td>0.325700</td>\n",
       "      <td>0.538800</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.340600</td>\n",
       "      <td>0.515848</td>\n",
       "      <td>0.614000</td>\n",
       "      <td>0.328800</td>\n",
       "      <td>0.542300</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.332100</td>\n",
       "      <td>0.514266</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>0.324300</td>\n",
       "      <td>0.539000</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.495500</td>\n",
       "      <td>0.513507</td>\n",
       "      <td>0.612300</td>\n",
       "      <td>0.326500</td>\n",
       "      <td>0.539700</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.526700</td>\n",
       "      <td>0.507800</td>\n",
       "      <td>0.615800</td>\n",
       "      <td>0.331000</td>\n",
       "      <td>0.544600</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.404200</td>\n",
       "      <td>0.506377</td>\n",
       "      <td>0.617900</td>\n",
       "      <td>0.333300</td>\n",
       "      <td>0.545700</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.425600</td>\n",
       "      <td>0.502965</td>\n",
       "      <td>0.618300</td>\n",
       "      <td>0.334500</td>\n",
       "      <td>0.547600</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.350700</td>\n",
       "      <td>0.520453</td>\n",
       "      <td>0.615200</td>\n",
       "      <td>0.332700</td>\n",
       "      <td>0.544400</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.278700</td>\n",
       "      <td>0.520350</td>\n",
       "      <td>0.615800</td>\n",
       "      <td>0.330700</td>\n",
       "      <td>0.544500</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.253400</td>\n",
       "      <td>0.518049</td>\n",
       "      <td>0.618400</td>\n",
       "      <td>0.336200</td>\n",
       "      <td>0.548200</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.346900</td>\n",
       "      <td>0.518024</td>\n",
       "      <td>0.618700</td>\n",
       "      <td>0.337000</td>\n",
       "      <td>0.547600</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.401900</td>\n",
       "      <td>0.513802</td>\n",
       "      <td>0.621100</td>\n",
       "      <td>0.339900</td>\n",
       "      <td>0.551000</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.381600</td>\n",
       "      <td>0.506862</td>\n",
       "      <td>0.622900</td>\n",
       "      <td>0.340400</td>\n",
       "      <td>0.552000</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.272700</td>\n",
       "      <td>0.524605</td>\n",
       "      <td>0.621900</td>\n",
       "      <td>0.339800</td>\n",
       "      <td>0.551900</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.276500</td>\n",
       "      <td>0.532190</td>\n",
       "      <td>0.620700</td>\n",
       "      <td>0.339500</td>\n",
       "      <td>0.550400</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.213500</td>\n",
       "      <td>0.525991</td>\n",
       "      <td>0.620100</td>\n",
       "      <td>0.339400</td>\n",
       "      <td>0.549500</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.239100</td>\n",
       "      <td>0.525771</td>\n",
       "      <td>0.620600</td>\n",
       "      <td>0.340100</td>\n",
       "      <td>0.550200</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.299900</td>\n",
       "      <td>0.524869</td>\n",
       "      <td>0.620500</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.549800</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.252100</td>\n",
       "      <td>0.523104</td>\n",
       "      <td>0.622500</td>\n",
       "      <td>0.341000</td>\n",
       "      <td>0.552000</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.270400</td>\n",
       "      <td>0.545691</td>\n",
       "      <td>0.620700</td>\n",
       "      <td>0.340300</td>\n",
       "      <td>0.550600</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.422300</td>\n",
       "      <td>0.545758</td>\n",
       "      <td>0.619000</td>\n",
       "      <td>0.337500</td>\n",
       "      <td>0.548600</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.265500</td>\n",
       "      <td>0.542587</td>\n",
       "      <td>0.623300</td>\n",
       "      <td>0.342200</td>\n",
       "      <td>0.552100</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>0.336000</td>\n",
       "      <td>0.541208</td>\n",
       "      <td>0.622000</td>\n",
       "      <td>0.341100</td>\n",
       "      <td>0.551100</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.542207</td>\n",
       "      <td>0.622700</td>\n",
       "      <td>0.343100</td>\n",
       "      <td>0.552900</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>0.215200</td>\n",
       "      <td>0.539547</td>\n",
       "      <td>0.623300</td>\n",
       "      <td>0.344400</td>\n",
       "      <td>0.553900</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>0.147000</td>\n",
       "      <td>0.559324</td>\n",
       "      <td>0.622900</td>\n",
       "      <td>0.343300</td>\n",
       "      <td>0.553400</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.258600</td>\n",
       "      <td>0.564043</td>\n",
       "      <td>0.621900</td>\n",
       "      <td>0.341900</td>\n",
       "      <td>0.551900</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>0.312000</td>\n",
       "      <td>0.561143</td>\n",
       "      <td>0.621000</td>\n",
       "      <td>0.341700</td>\n",
       "      <td>0.551200</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94000</td>\n",
       "      <td>0.279000</td>\n",
       "      <td>0.560756</td>\n",
       "      <td>0.621600</td>\n",
       "      <td>0.343700</td>\n",
       "      <td>0.552300</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>0.301900</td>\n",
       "      <td>0.561269</td>\n",
       "      <td>0.622800</td>\n",
       "      <td>0.345000</td>\n",
       "      <td>0.553400</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98000</td>\n",
       "      <td>0.185100</td>\n",
       "      <td>0.561305</td>\n",
       "      <td>0.623600</td>\n",
       "      <td>0.345700</td>\n",
       "      <td>0.554500</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.241900</td>\n",
       "      <td>0.558134</td>\n",
       "      <td>0.623700</td>\n",
       "      <td>0.346400</td>\n",
       "      <td>0.555100</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102000</td>\n",
       "      <td>0.252000</td>\n",
       "      <td>0.581107</td>\n",
       "      <td>0.622700</td>\n",
       "      <td>0.344300</td>\n",
       "      <td>0.553100</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104000</td>\n",
       "      <td>0.226200</td>\n",
       "      <td>0.585767</td>\n",
       "      <td>0.621700</td>\n",
       "      <td>0.344200</td>\n",
       "      <td>0.552100</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106000</td>\n",
       "      <td>0.228000</td>\n",
       "      <td>0.583010</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>0.345900</td>\n",
       "      <td>0.554600</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108000</td>\n",
       "      <td>0.168200</td>\n",
       "      <td>0.579663</td>\n",
       "      <td>0.623900</td>\n",
       "      <td>0.346400</td>\n",
       "      <td>0.554900</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>0.177800</td>\n",
       "      <td>0.580975</td>\n",
       "      <td>0.623500</td>\n",
       "      <td>0.346000</td>\n",
       "      <td>0.554400</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112000</td>\n",
       "      <td>0.283900</td>\n",
       "      <td>0.579603</td>\n",
       "      <td>0.624100</td>\n",
       "      <td>0.347400</td>\n",
       "      <td>0.555100</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114000</td>\n",
       "      <td>0.191100</td>\n",
       "      <td>0.595194</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>0.347500</td>\n",
       "      <td>0.554800</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116000</td>\n",
       "      <td>0.137700</td>\n",
       "      <td>0.595220</td>\n",
       "      <td>0.624300</td>\n",
       "      <td>0.347000</td>\n",
       "      <td>0.555000</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118000</td>\n",
       "      <td>0.156300</td>\n",
       "      <td>0.596153</td>\n",
       "      <td>0.624600</td>\n",
       "      <td>0.348400</td>\n",
       "      <td>0.555900</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>0.205600</td>\n",
       "      <td>0.595303</td>\n",
       "      <td>0.624800</td>\n",
       "      <td>0.348400</td>\n",
       "      <td>0.555800</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.596032</td>\n",
       "      <td>0.624500</td>\n",
       "      <td>0.348300</td>\n",
       "      <td>0.555600</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124000</td>\n",
       "      <td>0.174700</td>\n",
       "      <td>0.594579</td>\n",
       "      <td>0.625200</td>\n",
       "      <td>0.348700</td>\n",
       "      <td>0.556200</td>\n",
       "      <td>124.277900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=OUT_DIR,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=2000,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=2,\n",
    "    report_to='tensorboard',\n",
    "    learning_rate=0.0001,\n",
    "    dataloader_num_workers=4\n",
    ")\n",
    " \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    " \n",
    "history = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/home/mohan/infy/models/fine_tuned_Text_Summ/saved/tokenizer_config.json',\n",
       " '/home/mohan/infy/models/fine_tuned_Text_Summ/saved/special_tokens_map.json',\n",
       " '/home/mohan/infy/models/fine_tuned_Text_Summ/saved/vocab.json',\n",
       " '/home/mohan/infy/models/fine_tuned_Text_Summ/saved/merges.txt',\n",
       " '/home/mohan/infy/models/fine_tuned_Text_Summ/saved/added_tokens.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"/home/mohan/infy/models/fine_tuned_Text_Summ/saved\")\n",
    "tokenizer.save_pretrained(\"/home/mohan/infy/models/fine_tuned_Text_Summ/saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "infosys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
